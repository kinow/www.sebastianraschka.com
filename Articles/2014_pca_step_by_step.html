<!DOCTYPE html>
<html>
<head>
		<!--META-->
		<meta name="description" content="Showing how to implement a Principal Component Analysis (PCA) in Python and explaining it step by step.">
		<meta name="Author" content="Sebastian Raschka">
		<title>Principal Component Analysis step by step</title>

		<!-- jQuery -->
		<script type="text/javascript" src="http://code.jquery.com/jquery-latest.js"></script>
		<script type="text/javascript" src="../jQuery/main_app.js"></script>
		<script type="text/javascript" src=" ../jQuery/index.js"></script>

		<!-- CSS -->
		<link rel="stylesheet" type="text/css" href="../CSS/main.css" media="screen" />
		<link rel="stylesheet" type="text/css" href="../CSS/content.css" media="screen" />
</head>
<body>

	<div id ='container' >

		<div id ='header_new'>
		             
				<a href = "../index.html"><img src="../Images/title_text.png" style="border-style:none"/></a>	
				<hr>
                <a href="../rss_feed.xml"><img src="../Images/rss_logo_black_1.png" align="left" style="margin-left:430px"></a>

				<div align="right" style="padding-right:20px">
					<a href = "../articles.html" class ="menubar"><font color="#6495ED">blog</font></a> &#8226;
					<a href = "../webapps.html" class ="menubar">webapps</a> &#8226;
					<a href = "../software.html" class ="menubar">software</a> &#8226;
					<a href = "../publications.html" class ="menubar">books</a> &#8226;
					<a href = "../contact.html" class ="menubar">about+contact</a> 
					 
				</div>
				<hr>
		</div> <!--close header -->
	</div><!--close container -->


		<div id ='content'>
			
		


			<div id ='main'>

                <h2 class="article_top">Implementing a Principal Component Analysis (PCA) in Python step by step</h2>
					<p id="article_header"><em>-- written by Sebastian Raschka</em> on April 13, 2014</p>

	<!-- START Twitter API -->
        		 <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://bit.ly/Rg1KWe" 
                counturl="http://groups.google.com/group/ZEsjUY"
        		 data-count="none" data-via="rasbt"
        		 data-text="Implementing a Principal Component Analysis (PCA) in Python step by step" data-lang="en">Tweet</a>

        		<script>!function(d,s,id){var
        		js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="https://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
	<!-- END Twitter API --> <br> 
 

<style type="text/css">
code {
   font-size: 13.5px;
   color: #000099;
}
</style>

				<div class="article" style="font-size: 16px;">
		<div style="padding-right:70px">
		  <p style="line-height:1.7"><strong>

In this article I want to explain how a Principal Component Analysis (PCA) works by implementing it in Python step by step. At the end we will compare the results
to the more convenient Python <code>PCA()</code>classes that are available through the popular <code>matplotlib</code> and <code>scipy</code> libraries and discuss how they differ.
</strong></p></div>
<br>
<br>
<p>
<br>
<h1>Sections</h1>

<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#sample_data">Generating 3-dimensional sample data</a></li>
<li><a href="#gen_data">The step by step approach</a>

<ul>
<li>1.&nbsp;<a href="#drop_labels">Taking the whole dataset ignoring the class labels</a></li>
<li>2.&nbsp;<a href="#mean_vec"> Compute the <em>d</em>-dimensional mean vector</a></li>
<li>3.&nbsp;<a href="#sc_matrix">Computing the scatter matrix (alternatively, the covariance matrix)</a></li>
<li>4.&nbsp;<a href="#eig_vec">Computing eigenvectors and corresponding eigenvalues</a></li>
<li>5.&nbsp;<a href="#sort_eig">Ranking and choosing <em>k</em> eigenvectors</a></li>
<li>6.&nbsp;<a href="#transform">Transforming the samples onto the new subspace</a></li>
</ul>
</li>
<li><a href="#mat_pca">Using the <code>PCA()</code> class from the <code>matplotlib.mlab</code> library</a>

<ul>
<li>&nbsp;<a href="#diff_mat_pca">Differences between the step by step approach and matplotlib.mlab.PCA()</a></li>
</ul>
</li>
<li><a href="#sklearn_pca">Using the <code>PCA()</code> class from the sklearn.decomposition library to confirm our results</a></li>
</ul>


<p><a name="introduction"></a></p>

<h1>Introduction</h1>

<p>The main purposes of a principal component analysis are the analysis of data to identify patterns and finding patterns to reduce the dimensions of the dataset with minimal loss of information.</p>

<p>Here, our desired outcome of the principal component analysis is to project a feature space (our dataset consisting of <em>n</em> x <em>d</em>-dimensional samples) onto a smaller subspace that represents our data "well". A possible application would be a pattern classification task, where we want to reduce the computational costs and the error of parameter estimation by reducing the number of dimensions of our feature space by extracting a subspace that describes our data "best".</p>

<hr>


<p><strong>About the notation:</strong><br/>
In the following sections, we will use a bold-face and lower-case letters for denoting column vectors vectors (e.g., <strong>e</strong>) and bold-face upper-case letters for matrices (e.g., <strong>W</strong>)</p>

<hr>


<h3>Principal Component Analysis (PCA) Vs. Multiple Discriminant Analysis (MDA)</h3>

<p>Both Multiple Discriminant Analysis (MDA) and Principal Component Analysis (PCA) are linear transformation methods and closely related to each other. In PCA, we are interested to find the directions (components) that maximize the variance in our dataset, where in MDA, we are additionally interested to find the directions that maximize the separation (or discrimination) between different classes (for example, in pattern classification problems where our dataset consists of multiple classes. In contrast two PCA, which ignores the class labels).<br/>
<strong><em>In other words, via PCA, we are projecting the entire set of data (without class labels) onto a different subspace, and in MDA, we are trying to determine a suitable subspace to distinguish between patterns that belong to different classes. Or, roughly speaking in PCA we are trying to find the axes with maximum variances where the data is most spread (within a class, since PCA treats the whole data set as one class), and in MDA we are additionally maximizing the spread between classes. </em></strong><br/>
In typical pattern recognition problems, a PCA is often followed by an MDA.</p>

<h4>What is a "good" subspace?</h4>

<p>Let's assume that our goal is to reduce the dimensions of a <em>d</em>-dimensional dataset by projecting it onto a <em>k</em>-dimensional subspace (where <em>k &lt; d</em>).
So, how do we know what size we should choose for <em>k</em>, and how do we know if we have a feature space that represents our data "well"?<br/>
Later, we will compute eigenvectors (the components) from our data set and collect them in a so-called scatter-matrix (or alternatively calculate them from the covariance matrix). Each of those eigenvectors is associated with an eigenvalue, which tell us about the "length" or "magnitude" of the eigenvectors. If we observe that all the eigenvalues are of very similar magnitude, this is a good indicator that our data is already in a "good" subspace. Or if some of the eigenvalues are much much higher than others, we might be interested in keeping only those eigenvectors with the much larger eigenvalues, since they contain more information about our data distribution. Vice versa, eigenvalues that are close to 0 are less informative and we might consider in dropping those when we construct the new feature subspace.</p>

<h3>Summarizing the PCA approach</h3>

<p>Listed below are the 6 general steps for performing a principal component analysis, which we will investigate in the following sections.</p>

<ol>
<li><a href="#drop_labels"> Take the whole dataset consisting of <em>d</em>-dimensional samples ignoring the class labels</a></li>
<li><a href="#mean_vec"> Compute the <em>d</em>-dimensional mean vector</a> (i.e., the means for every dimension of the whole dataset)</li>
<li><a href="#sc_matrix">Compute the scatter matrix (alternatively, the covariance matrix) of the whole data set</a></li>
<li><a href="#eig_vec">Compute eigenvectors and corresponding eigenvalues </a></li>
<li><a href="#sort_eig">Sort the eigenvectors by decreasing eigenvalues and choose <em>k</em> eigenvectors with the largest eigenvalues to form a <em>d</em> x <em>k</em>  dimensional matrix <strong>W</strong></a>  (where every column represents an eigenvector)</li>
<li><a href="#transform">Use this <em>d</em> x <em>k</em> eigenvector matrix to transform the samples onto the new subspace.</a> This can be summarized by the mathematical equation:<br/>
<img src='../Images/pca_equ_1.png'><br/>
(where <strong>x</strong> is a <em>d</em> x 1 -dimensional vector representing one sample, and <em>y</em> is the transformed <strong>k</strong> x 1 -dimensional sample in the new subspace.)</li>
</ol>


<p><a name="sample_data"></a></p>

<h1>Generating some 3-dimensional sample data</h1>

<p>For the following example, we will generate 40x3-dimensional samples randomly drawn from a multivariate Gaussian distribution.  <br/>
Here, we will assume that the samples stem from two different classes, where one half (i.e., 20) samples of our data set are labeled &omega;1 (class 1) and the other half &omega;2 (class 2).<br/>
<img src='../Images/pca_equ_2.png'></p>

<h3>Why are we chosing a 3-dimensional sample?</h3>

<p>The problem of multi-dimensional data is its visualization, which would make it quite tough to follow our example principal component analysis (at least visually). We could also choose a 2-dimensional sample data set for the following examples, but since the goal of the PCA in an "Diminsionality Reduction" application is to drop at least one of the dimensions, I find it more intuitive and visually appealing to start with a 3-dimensional dataset that we reduce to an 2-dimensional dataset by dropping 1 dimension.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">import</span> <span style="color: #0e84b5; font-weight: bold">numpy</span> <span style="color: #008800; font-weight: bold">as</span> <span style="color: #0e84b5; font-weight: bold">np</span>

np<span style="color: #333333">.</span>random<span style="color: #333333">.</span>seed(<span style="color: #0000DD; font-weight: bold">234234782384239784</span>) <span style="color: #888888"># random seed for consistency</span>

<span style="color: #888888"># A reader pointed out that Python 2.7 would raise a</span>
<span style="color: #888888"># &quot;ValueError: object of too small depth for desired array&quot;.</span>
<span style="color: #888888"># This can be avoided by choosing a smaller random seed, e.g. 1</span>
<span style="color: #888888"># or by completely omitting this line, since I just used the random seed for</span>
<span style="color: #888888"># consistency.</span>

mu_vec1 <span style="color: #333333">=</span> np<span style="color: #333333">.</span>array([<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>])
cov_mat1 <span style="color: #333333">=</span> np<span style="color: #333333">.</span>array([[<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>],[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">0</span>],[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">1</span>]])
class1_sample <span style="color: #333333">=</span> np<span style="color: #333333">.</span>random<span style="color: #333333">.</span>multivariate_normal(mu_vec1, cov_mat1, <span style="color: #0000DD; font-weight: bold">20</span>)<span style="color: #333333">.</span>T
<span style="color: #008800; font-weight: bold">assert</span> class1_sample<span style="color: #333333">.</span>shape <span style="color: #333333">==</span> (<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">20</span>), <span style="background-color: #fff0f0">&quot;The matrix has not the dimensions 3x20&quot;</span>

mu_vec2 <span style="color: #333333">=</span> np<span style="color: #333333">.</span>array([<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">1</span>])
cov_mat2 <span style="color: #333333">=</span> np<span style="color: #333333">.</span>array([[<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>],[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">0</span>],[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">1</span>]])
class2_sample <span style="color: #333333">=</span> np<span style="color: #333333">.</span>random<span style="color: #333333">.</span>multivariate_normal(mu_vec2, cov_mat2, <span style="color: #0000DD; font-weight: bold">20</span>)<span style="color: #333333">.</span>T
<span style="color: #008800; font-weight: bold">assert</span> class1_sample<span style="color: #333333">.</span>shape <span style="color: #333333">==</span> (<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">20</span>), <span style="background-color: #fff0f0">&quot;The matrix has not the dimensions 3x20&quot;</span>
</pre></div>


<p>Using the code above, we created two 3x20-datasets - one dataset for each class &omega;1 and &omega;2 -<br/>
where each column can be pictured as a 3-dimensional vector<br/>
<img src='../Images/pca_equ_3.png'><br/>
so that our dataset will have the form <br/>
<img src='../Images/pca_equ_4.png'></p>

<p>Just to get a rough idea how the samples of our two classes &omega;1 and &omega;2 are distributed, let us plot them in a 3D scatter plot.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">matplotlib</span> <span style="color: #008800; font-weight: bold">import</span> pyplot <span style="color: #008800; font-weight: bold">as</span> plt
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008800; font-weight: bold">import</span> Axes3D
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008800; font-weight: bold">import</span> proj3d

fig <span style="color: #333333">=</span> plt<span style="color: #333333">.</span>figure(figsize<span style="color: #333333">=</span>(<span style="color: #0000DD; font-weight: bold">8</span>,<span style="color: #0000DD; font-weight: bold">8</span>))
ax <span style="color: #333333">=</span> fig<span style="color: #333333">.</span>add_subplot(<span style="color: #0000DD; font-weight: bold">111</span>, projection<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;3d&#39;</span>)
plt<span style="color: #333333">.</span>rcParams[<span style="background-color: #fff0f0">&#39;legend.fontsize&#39;</span>] <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">10</span>   
ax<span style="color: #333333">.</span>plot(class1_sample[<span style="color: #0000DD; font-weight: bold">0</span>,:], class1_sample[<span style="color: #0000DD; font-weight: bold">1</span>,:],\ 
    class1_sample[<span style="color: #0000DD; font-weight: bold">2</span>,:], <span style="background-color: #fff0f0">&#39;o&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">8</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;blue&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class1&#39;</span>)
ax<span style="color: #333333">.</span>plot(class2_sample[<span style="color: #0000DD; font-weight: bold">0</span>,:], class2_sample[<span style="color: #0000DD; font-weight: bold">1</span>,:],\ 
    class2_sample[<span style="color: #0000DD; font-weight: bold">2</span>,:], <span style="background-color: #fff0f0">&#39;^&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">8</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;red&#39;</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class2&#39;</span>)

plt<span style="color: #333333">.</span>title(<span style="background-color: #fff0f0">&#39;Samples for class 1 and class 2&#39;</span>)
ax<span style="color: #333333">.</span>legend(loc<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;upper right&#39;</span>)
plt<span style="color: #333333">.</span>draw()
plt<span style="color: #333333">.</span>show()
</pre></div>


<p><img src='../Images/pca_plot_1.png'></p>

<p><a name='drop_labels'></a></p>

<h1>1. Taking the whole dataset ignoring the class labels</h1>

<p>Because we don't need class labels for the PCA analysis, let us merge the samples for our 2 classes into one 3x40-dimensional array.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">all_samples <span style="color: #333333">=</span> np<span style="color: #333333">.</span>concatenate((class1_sample, class2_sample), axis<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">1</span>)
<span style="color: #008800; font-weight: bold">assert</span> all_samples<span style="color: #333333">.</span>shape <span style="color: #333333">==</span> (<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">40</span>), <span style="background-color: #fff0f0">&quot;The matrix has not the dimensions 3x40&quot;</span>
</pre></div>


<p><a name='mean_vec'></a></p>

<h1>2. Computing the d-dimensional mean vector</h1>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">mean_x <span style="color: #333333">=</span> np<span style="color: #333333">.</span>mean(all_samples[<span style="color: #0000DD; font-weight: bold">0</span>,:])
mean_y <span style="color: #333333">=</span> np<span style="color: #333333">.</span>mean(all_samples[<span style="color: #0000DD; font-weight: bold">1</span>,:])
mean_z <span style="color: #333333">=</span> np<span style="color: #333333">.</span>mean(all_samples[<span style="color: #0000DD; font-weight: bold">2</span>,:])

mean_vector <span style="color: #333333">=</span> np<span style="color: #333333">.</span>array([[mean_x],[mean_y],[mean_z]])

<span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Mean Vector:</span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">&#39;</span>, mean_vector)


>> Mean Vector:
 [[ <span style="color: #6600EE; font-weight: bold">0.50576644</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.30186591</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.76459177</span>]]
</pre></div>


<p><a name="mean_vec"></a></p>

<h1>3. a) Computing the Scatter Matrix</h1>

<p><img src='../Images/pca_equ_5.png'></p>

<pre>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">scatter_matrix <span style="color: #333333">=</span> np<span style="color: #333333">.</span>zeros((<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">3</span>))
<span style="color: #008800; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">range</span>(all_samples<span style="color: #333333">.</span>shape[<span style="color: #0000DD; font-weight: bold">1</span>]):
    scatter_matrix <span style="color: #333333">+=</span> (all_samples[:,i]<span style="color: #333333">.</span>reshape(<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">1</span>)\
         <span style="color: #333333">-</span> mean_vector)<span style="color: #333333">.</span>dot((all_samples[:,i]<span style="color: #333333">.</span>reshape(<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">1</span>) <span style="color: #333333">-</span> mean_vector)<span style="color: #333333">.</span>T)
<span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Scatter Matrix:</span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">&#39;</span>, scatter_matrix)  



>> Scatter Matrix:
 [[ <span style="color: #6600EE; font-weight: bold">48.91593255</span>   <span style="color: #6600EE; font-weight: bold">7.11744916</span>   <span style="color: #6600EE; font-weight: bold">7.20810281</span>]
 [  <span style="color: #6600EE; font-weight: bold">7.11744916</span>  <span style="color: #6600EE; font-weight: bold">37.92902984</span>   <span style="color: #6600EE; font-weight: bold">2.7370493</span> ]
 [  <span style="color: #6600EE; font-weight: bold">7.20810281</span>   <span style="color: #6600EE; font-weight: bold">2.7370493</span>   <span style="color: #6600EE; font-weight: bold">35.6363759</span> ]]
</pre></div>
</pre>


<h1>3. b) Computing the Covariance Matrix (alternatively to the scatter matrix)</h1>

<p>Alternatively, instead of calculating the scatter matrix, we could also calculate the covariance matrix using the in-built <code>numpy.cov()</code> function.  The equations for the covariance matrix and scatter matrix are very similar, the only difference is, that we use the scaling factor
<img src='../Images/pca_equ_6.png'>
for the covariance matrix. Thus, their <strong><em>eigenspaces</em></strong> will be identical (identical eigenvectors, only the eigenvalues are scaled differently by a constant factor).</p>

<p><img src='../Images/pca_equ_7.png'></p>

<pre>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">cov_mat <span style="color: #333333">=</span> np<span style="color: #333333">.</span>cov([all_samples[<span style="color: #0000DD; font-weight: bold">0</span>,:],all_samples[<span style="color: #0000DD; font-weight: bold">1</span>,:],all_samples[<span style="color: #0000DD; font-weight: bold">2</span>,:]])
<span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Covariance Matrix:</span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">&#39;</span>, cov_mat)


>> Covariance Matrix:
 [[ <span style="color: #6600EE; font-weight: bold">1.25425468</span>  <span style="color: #6600EE; font-weight: bold">0.1824987</span>   <span style="color: #6600EE; font-weight: bold">0.18482315</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.1824987</span>   <span style="color: #6600EE; font-weight: bold">0.97253923</span>  <span style="color: #6600EE; font-weight: bold">0.07018075</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.18482315</span>  <span style="color: #6600EE; font-weight: bold">0.07018075</span>  <span style="color: #6600EE; font-weight: bold">0.91375323</span>]]
</pre></div>
</pre>


<p><a name="eig_vec"></a></p>

<h1>4. Computing eigenvectors and corresponding eigenvalues</h1>

<p>To show that the eigenvectors are indeed identical whether we derived them from the scatter or the covariance matrix, let us put an <code>assert</code> statement into the code. Also, we will see that the eigenvalues were indeed scaled by the factor 39 when we derived it from the scatter matrix.</p>

<pre>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #888888"># eigenvectors and eigenvalues for the from the scatter matrix</span>
eig_val_sc, eig_vec_sc <span style="color: #333333">=</span> np<span style="color: #333333">.</span>linalg<span style="color: #333333">.</span>eig(scatter_matrix)

<span style="color: #888888"># eigenvectors and eigenvalues for the from the covariance matrix</span>
eig_val_cov, eig_vec_cov <span style="color: #333333">=</span> np<span style="color: #333333">.</span>linalg<span style="color: #333333">.</span>eig(cov_mat)

<span style="color: #008800; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">range</span>(<span style="color: #007020">len</span>(eig_val_sc)):
    eigvec_sc <span style="color: #333333">=</span> eig_vec_sc[:,i]<span style="color: #333333">.</span>reshape(<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">3</span>)<span style="color: #333333">.</span>T
    eigvec_cov <span style="color: #333333">=</span> eig_vec_cov[:,i]<span style="color: #333333">.</span>reshape(<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">3</span>)<span style="color: #333333">.</span>T
    <span style="color: #008800; font-weight: bold">assert</span> eigvec_sc<span style="color: #333333">.</span>all() <span style="color: #333333">==</span> eigvec_cov<span style="color: #333333">.</span>all(), <span style="background-color: #fff0f0">&#39;Eigenvectors are not identical&#39;</span>
    
    <span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Eigenvector {}: </span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">{}&#39;</span><span style="color: #333333">.</span>format(i<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">1</span>, eigvec_sc))
    <span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Eigenvalue {} from scatter matrix: {}&#39;</span><span style="color: #333333">.</span>format(i<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">1</span>, eig_val_sc[i]))
    <span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Eigenvalue {} from covariance matrix: {}&#39;</span><span style="color: #333333">.</span>format(i<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">1</span>, eig_val_cov[i]))
    <span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Scaling factor: &#39;</span>, eig_val_sc[i]<span style="color: #333333">/</span>eig_val_cov[i])
    <span style="color: #007020">print</span>(<span style="color: #0000DD; font-weight: bold">40</span> <span style="color: #333333">*</span> <span style="background-color: #fff0f0">&#39;-&#39;</span>)
    
    
    
>> Eigenvector <span style="color: #0000DD; font-weight: bold">1</span>: 
[[<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.84190486</span>]
 [<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.39978877</span>]
 [<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.36244329</span>]]
Eigenvalue <span style="color: #0000DD; font-weight: bold">1</span> <span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">scatter</span> matrix: <span style="color: #6600EE; font-weight: bold">55.398855957302445</span>
Eigenvalue <span style="color: #0000DD; font-weight: bold">1</span> <span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">covariance</span> matrix: <span style="color: #6600EE; font-weight: bold">1.4204834860846791</span>
Scaling factor:  <span style="color: #6600EE; font-weight: bold">39.0</span>
<span style="color: #333333">----------------------------------------</span>
Eigenvector <span style="color: #0000DD; font-weight: bold">2</span>: 
[[<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.44565232</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.13637858</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.88475697</span>]]
Eigenvalue <span style="color: #0000DD; font-weight: bold">2</span> <span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">scatter</span> matrix: <span style="color: #6600EE; font-weight: bold">32.42754801292286</span>
Eigenvalue <span style="color: #0000DD; font-weight: bold">2</span> <span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">covariance</span> matrix: <span style="color: #6600EE; font-weight: bold">0.8314755900749456</span>
Scaling factor:  <span style="color: #6600EE; font-weight: bold">39.0</span>
<span style="color: #333333">----------------------------------------</span>
Eigenvector <span style="color: #0000DD; font-weight: bold">3</span>: 
[[ <span style="color: #6600EE; font-weight: bold">0.30428639</span>]
 [<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.90640489</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.29298458</span>]]
Eigenvalue <span style="color: #0000DD; font-weight: bold">3</span> <span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">scatter</span> matrix: <span style="color: #6600EE; font-weight: bold">34.65493432806495</span>
Eigenvalue <span style="color: #0000DD; font-weight: bold">3</span> <span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">covariance</span> matrix: <span style="color: #6600EE; font-weight: bold">0.8885880596939733</span>
Scaling factor:  <span style="color: #6600EE; font-weight: bold">39.0</span>
<span style="color: #333333">----------------------------------------</span>
</pre></div>
</pre>


<p></p>

<h3>Checking the eigenvector-eigenvalue calculation</h3>

<p>Let us quickly check that the eigenvector-eigenvalue calculation is correct and satisfy the equation
<img src='../Images/pca_equ_8.png'></p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">range</span>(<span style="color: #007020">len</span>(eig_val_sc)):
    eigv <span style="color: #333333">=</span> eig_vec_sc[:,i]<span style="color: #333333">.</span>reshape(<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">3</span>)<span style="color: #333333">.</span>T
    np<span style="color: #333333">.</span>testing<span style="color: #333333">.</span>assert_array_almost_equal(scatter_matrix<span style="color: #333333">.</span>dot(eigv),\
            eig_val_sc[i] <span style="color: #333333">*</span> eigv, decimal<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">6</span>,\
            err_msg<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;&#39;</span>, verbose<span style="color: #333333">=</span><span style="color: #008800; font-weight: bold">True</span>)
</pre></div>


<p></p>

<h3>Visualizing the eigenvectors</h3>

<p>And before we move on to the next step, just to satisfy our own curiosity, we plot the eigenvectors centered at the sample mean.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">matplotlib</span> <span style="color: #008800; font-weight: bold">import</span> pyplot <span style="color: #008800; font-weight: bold">as</span> plt
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008800; font-weight: bold">import</span> Axes3D
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008800; font-weight: bold">import</span> proj3d
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">matplotlib.patches</span> <span style="color: #008800; font-weight: bold">import</span> FancyArrowPatch


<span style="color: #008800; font-weight: bold">class</span> <span style="color: #BB0066; font-weight: bold">Arrow3D</span>(FancyArrowPatch):
    <span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">__init__</span>(<span style="color: #007020">self</span>, xs, ys, zs, <span style="color: #333333">*</span>args, <span style="color: #333333">**</span>kwargs):
        FancyArrowPatch<span style="color: #333333">.</span>__init__(<span style="color: #007020">self</span>, (<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>), (<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>), <span style="color: #333333">*</span>args, <span style="color: #333333">**</span>kwargs)
        <span style="color: #007020">self</span><span style="color: #333333">.</span>_verts3d <span style="color: #333333">=</span> xs, ys, zs

    <span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">draw</span>(<span style="color: #007020">self</span>, renderer):
        xs3d, ys3d, zs3d <span style="color: #333333">=</span> <span style="color: #007020">self</span><span style="color: #333333">.</span>_verts3d
        xs, ys, zs <span style="color: #333333">=</span> proj3d<span style="color: #333333">.</span>proj_transform(xs3d, ys3d, zs3d, renderer<span style="color: #333333">.</span>M)
        <span style="color: #007020">self</span><span style="color: #333333">.</span>set_positions((xs[<span style="color: #0000DD; font-weight: bold">0</span>],ys[<span style="color: #0000DD; font-weight: bold">0</span>]),(xs[<span style="color: #0000DD; font-weight: bold">1</span>],ys[<span style="color: #0000DD; font-weight: bold">1</span>]))
        FancyArrowPatch<span style="color: #333333">.</span>draw(<span style="color: #007020">self</span>, renderer)

fig <span style="color: #333333">=</span> plt<span style="color: #333333">.</span>figure(figsize<span style="color: #333333">=</span>(<span style="color: #0000DD; font-weight: bold">7</span>,<span style="color: #0000DD; font-weight: bold">7</span>))
ax <span style="color: #333333">=</span> fig<span style="color: #333333">.</span>add_subplot(<span style="color: #0000DD; font-weight: bold">111</span>, projection<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;3d&#39;</span>)

ax<span style="color: #333333">.</span>plot(all_samples[<span style="color: #0000DD; font-weight: bold">0</span>,:], all_samples[<span style="color: #0000DD; font-weight: bold">1</span>,:],\
    all_samples[<span style="color: #0000DD; font-weight: bold">2</span>,:], <span style="background-color: #fff0f0">&#39;o&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">8</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;green&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.2</span>)
ax<span style="color: #333333">.</span>plot([mean_x], [mean_y], [mean_z], <span style="background-color: #fff0f0">&#39;o&#39;</span>, \
    markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">10</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;red&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>)
<span style="color: #008800; font-weight: bold">for</span> v <span style="color: #000000; font-weight: bold">in</span> eig_vec_sc<span style="color: #333333">.</span>T:
    a <span style="color: #333333">=</span> Arrow3D([mean_x, v[<span style="color: #0000DD; font-weight: bold">0</span>]], [mean_y, v[<span style="color: #0000DD; font-weight: bold">1</span>]],\
        [mean_z, v[<span style="color: #0000DD; font-weight: bold">2</span>]], mutation_scale<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">20</span>, lw<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">3</span>, arrowstyle<span style="color: #333333">=</span><span style="background-color: #fff0f0">&quot;-|&gt;&quot;</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&quot;r&quot;</span>)
    ax<span style="color: #333333">.</span>add_artist(a)
ax<span style="color: #333333">.</span>set_xlabel(<span style="background-color: #fff0f0">&#39;x_values&#39;</span>)
ax<span style="color: #333333">.</span>set_ylabel(<span style="background-color: #fff0f0">&#39;y_values&#39;</span>)
ax<span style="color: #333333">.</span>set_zlabel(<span style="background-color: #fff0f0">&#39;z_values&#39;</span>)

plt<span style="color: #333333">.</span>title(<span style="background-color: #fff0f0">&#39;Eigenvectors&#39;</span>)

plt<span style="color: #333333">.</span>show()
</pre></div>


<p></p>

<p><img src='../Images/pca_plot_2.png'></p>

<p><a name="sort_eig"></a></p>

<h1>5.1. Sorting the eigenvectors by decreasing eigenvalues</h1>

<p>We started with the goal to reduce the dimensionality of our feature space, i.e., projecting the feature space via PCA onto a smaller subspace, where the eigenvectors will form the axes of this new feature subspace. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, which we can confirm by the following code:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">for</span> ev <span style="color: #000000; font-weight: bold">in</span> eig_vec_sc:
    numpy<span style="color: #333333">.</span>testing<span style="color: #333333">.</span>assert_array_almost_equal(<span style="color: #6600EE; font-weight: bold">1.0</span>, np<span style="color: #333333">.</span>linalg<span style="color: #333333">.</span>norm(ev))
    <span style="color: #888888"># instead of &#39;assert&#39; because of rounding errors</span>
</pre></div>


<p>So, in order to decide which eigenvector(s) we want to drop for our lower-dimensional subspace, we have to take a look at the corresponding eigenvalues of the eigenvectors. Roughly speaking, the eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data, and those are the ones we want to drop.<br/>
The common approach is to rank the eigenvectors from highest to lowest corresponding eigenvalue and choose the top <em>k</em> eigenvectors.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #888888"># Make a list of (eigenvalue, eigenvector) tuples</span>
eig_pairs <span style="color: #333333">=</span> [(np<span style="color: #333333">.</span>abs(eig_val_sc[i]), eig_vec_sc[:,i]) <span style="color: #008800; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">range</span>(<span style="color: #007020">len</span>(eig_val_sc))]

<span style="color: #888888"># Sort the (eigenvalue, eigenvector) tuples from high to low</span>
eig_pairs<span style="color: #333333">.</span>sort()
eig_pairs<span style="color: #333333">.</span>reverse()

<span style="color: #888888"># Visually confirm that the list is correctly sorted by decreasing eigenvalues</span>
<span style="color: #008800; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> eig_pairs:
    <span style="color: #007020">print</span>(i[<span style="color: #0000DD; font-weight: bold">0</span>])
    
>> 
<span style="color: #6600EE; font-weight: bold">55.3988559573</span>
<span style="color: #6600EE; font-weight: bold">34.6549343281</span>
<span style="color: #6600EE; font-weight: bold">32.4275480129</span>
</pre></div>


<p></p>

<h1>5.2. Choosing <em>k</em> eigenvectors with the largest eigenvalues</h1>

<p>For our simple example, where we are reducing a 3-dimensional feature space to a 2-dimensional feature subspace, we are combining the two eigenvectors with the highest eigenvalues to construct our <em>d</em> x <em>k</em>-dimensional eigenvector matrix <strong>W</strong>.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">matrix_w <span style="color: #333333">=</span> np<span style="color: #333333">.</span>hstack((eig_pairs[<span style="color: #0000DD; font-weight: bold">0</span>][<span style="color: #0000DD; font-weight: bold">1</span>]<span style="color: #333333">.</span>reshape(<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">1</span>), eig_pairs[<span style="color: #0000DD; font-weight: bold">1</span>][<span style="color: #0000DD; font-weight: bold">1</span>]<span style="color: #333333">.</span>reshape(<span style="color: #0000DD; font-weight: bold">3</span>,<span style="color: #0000DD; font-weight: bold">1</span>)))
<span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;Matrix W:</span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">&#39;</span>, matrix_w)


>> Matrix W:
 [[<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.84190486</span>  <span style="color: #6600EE; font-weight: bold">0.30428639</span>]
 [<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.39978877</span> <span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.90640489</span>]
 [<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.36244329</span>  <span style="color: #6600EE; font-weight: bold">0.29298458</span>]]
</pre></div>


<p></p>

<p><a name='transform'></a></p>

<h1>6. Transforming the samples onto the new subspace</h1>

<p>In the last step, we use the 2x40-dimensional matrix <strong>W</strong> that we just computed to transform our samples onto the new subspace via the equation<br/>
<img src='../Images/pca_equ_9.png'></p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">transformed <span style="color: #333333">=</span> matrix_w<span style="color: #333333">.</span>T<span style="color: #333333">.</span>dot(all_samples)
<span style="color: #008800; font-weight: bold">assert</span> transformed<span style="color: #333333">.</span>shape <span style="color: #333333">==</span> (<span style="color: #0000DD; font-weight: bold">2</span>,<span style="color: #0000DD; font-weight: bold">40</span>), <span style="background-color: #fff0f0">&quot;The matrix is not 2x40 dimensional.&quot;</span>


plt<span style="color: #333333">.</span>plot(transformed[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>], transformed[<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>],\
     <span style="background-color: #fff0f0">&#39;o&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;blue&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class1&#39;</span>)
plt<span style="color: #333333">.</span>plot(transformed[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>], transformed[<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>],
     <span style="background-color: #fff0f0">&#39;^&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;red&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class2&#39;</span>)
plt<span style="color: #333333">.</span>xlim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>ylim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>xlabel(<span style="background-color: #fff0f0">&#39;x_values&#39;</span>)
plt<span style="color: #333333">.</span>ylabel(<span style="background-color: #fff0f0">&#39;y_values&#39;</span>)
plt<span style="color: #333333">.</span>legend()
plt<span style="color: #333333">.</span>title(<span style="background-color: #fff0f0">&#39;Transformed samples with class labels&#39;</span>)

plt<span style="color: #333333">.</span>draw()
plt<span style="color: #333333">.</span>show()
</pre></div>


<p><img src='../Images/pca_plot_3.png'></p>

<p><a name="mat_pca"></a></p>

<h1>Using the PCA() class from the matplotlib.mlab library</h1>

<p>Now, that we have seen how a principal component analysis works, we can use the in-built <code>PCA()</code> class from the <code>matplotlib</code> library for our convenience in future applications.
Unfortunately, the original documentation (<a href="http://matplotlib.sourceforge.net/api/mlab_api.html#matplotlib.mlab.PCA">http://matplotlib.sourceforge.net/api/mlab_api.html#matplotlib.mlab.PCA</a>) is very sparse;<br/>
a better documentation can be found here: <a href="https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml">https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml</a>.</p>

<p>And the original code implementation of the <code>PCA()</code> class can be viewed at:<br/>
<a href="https://sourcegraph.com/github.com/matplotlib/matplotlib/symbols/python/lib/matplotlib/mlab/PCA">https://sourcegraph.com/github.com/matplotlib/matplotlib/symbols/python/lib/matplotlib/mlab/PCA</a></p>

<h4>Class attributes of <code>PCA()</code></h4>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">a : a centered unit sigma version of <span style="color: #007020">input</span> a

numrows, numcols: the dimensions of a

mu : a numdims array of means of a

sigma : a numdims array of atandard deviation of a

fracs : the proportion of variance of each of the principal components

Wt : the weight vector <span style="color: #008800; font-weight: bold">for</span> projecting a numdims point <span style="color: #000000; font-weight: bold">or</span> array into PCA space

Y : a projected into PCA space
</pre></div>

<p>Also, it has to be mentioned that the <code>PCA()</code> class expects a <code>np.array()</code> as input where: <code>'we assume data in a is organized with numrows&gt;numcols')</code>, so that we have to transpose our dataset.</p>

<p><code>matplotlib.mlab.PCA()</code> keeps all $d$-dimensions of the input dataset after the transformation (stored in the class attribute <code>PCA.Y</code>), and assuming that they are already ordered ("Since the PCA analysis orders the PC axes by descending importance in terms of describing the clustering, we see that fracs is a list of monotonically decreasing values.", <a href="https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml">https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml</a>) we just need to plot the first 2 columns if we are interested in projecting our 3-dimensional input dataset onto a 2-dimensional subspace.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">matplotlib.mlab</span> <span style="color: #008800; font-weight: bold">import</span> PCA <span style="color: #008800; font-weight: bold">as</span> mlabPCA
 
mlab_pca <span style="color: #333333">=</span> mlabPCA(all_samples<span style="color: #333333">.</span>T) 

<span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&#39;PC axes in terms of the measurement axes&#39;</span>\ 
        <span style="background-color: #fff0f0">&#39; scaled by the standard deviations:</span><span style="color: #666666; font-weight: bold; background-color: #fff0f0">\n</span><span style="background-color: #fff0f0">&#39;</span>,\
          mlab_pca<span style="color: #333333">.</span>Wt)

plt<span style="color: #333333">.</span>plot(mlab_pca<span style="color: #333333">.</span>Y[<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>,<span style="color: #0000DD; font-weight: bold">0</span>],mlab_pca<span style="color: #333333">.</span>Y[<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>,<span style="color: #0000DD; font-weight: bold">1</span>], <span style="background-color: #fff0f0">&#39;o&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>,\ 
        color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;blue&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class1&#39;</span>)
plt<span style="color: #333333">.</span>plot(mlab_pca<span style="color: #333333">.</span>Y[<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>,<span style="color: #0000DD; font-weight: bold">0</span>], mlab_pca<span style="color: #333333">.</span>Y[<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>,<span style="color: #0000DD; font-weight: bold">1</span>], <span style="background-color: #fff0f0">&#39;^&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>,\
        color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;red&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class2&#39;</span>)

plt<span style="color: #333333">.</span>xlabel(<span style="background-color: #fff0f0">&#39;x_values&#39;</span>)
plt<span style="color: #333333">.</span>ylabel(<span style="background-color: #fff0f0">&#39;y_values&#39;</span>)
plt<span style="color: #333333">.</span>xlim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>ylim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>legend()
plt<span style="color: #333333">.</span>title(<span style="background-color: #fff0f0">&#39;Transformed samples with class labels from matplotlib.mlab.PCA()&#39;</span>)

plt<span style="color: #333333">.</span>draw()
plt<span style="color: #333333">.</span>show()




>> PC axes <span style="color: #000000; font-weight: bold">in</span> terms of the measurement axes scaled by the standard deviations:
 [[ <span style="color: #6600EE; font-weight: bold">0.65043619</span>  <span style="color: #6600EE; font-weight: bold">0.53023618</span>  <span style="color: #6600EE; font-weight: bold">0.54385876</span>]
 [<span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.01692055</span>  <span style="color: #6600EE; font-weight: bold">0.72595458</span> <span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.68753447</span>]
 [ <span style="color: #6600EE; font-weight: bold">0.75937241</span> <span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.43799491</span> <span style="color: #333333">-</span><span style="color: #6600EE; font-weight: bold">0.48115902</span>]]
</pre></div>



<p><img src='../Images/pca_plot_4.png'></p>

<p><a name="_diff_mat_pca"></a></p>

<h2>Differences between the step by step approach and matplotlib.mlab.PCA()</h2>

<p>When we plot the transformed dataset onto the new 2-dimensional subspace, we observe that the scatter plots from our step by step approach and the <code>matplotlib.mlab.PCA()</code> class do not look identical. This is due to the fact that <code>matplotlib.mlab.PCA()</code> class <strong><em>scales the variables to unit variance</em></strong> prior to calculating the covariance matrices. This will/could eventually lead to different variances along the axes and affect the contribution of the variable to principal components.</p>

<p>One example where a scaling would make sense would be if one variable was measured in the unit <strong>inches</strong> where the other variable was measured in <strong>cm</strong>.<br/>
However, for our hypothetical example, we assume that both variables have the same (arbitrary) unit, so that we skipped the step of scaling the input data.</p>

<p><a name="sklearn_pca"> </a></p>

<h1>Using the PCA() class from the sklearn.decomposition library to confirm our results</h1>

<p>In order to make sure that we have not made a mistake in our step by step approach, we will use another library that doesn't rescale the input data by default.<br/>
Here, we will use the PCA class from the <code>scikit-learn</code> machine-learning library. The documentation can be found here:<br/>
<a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a>.</p>

<p>For our convenience, we can directly specify to how many components we want to reduce our input dataset via the <code>n_components</code> parameter.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">n_components : <span style="color: #007020">int</span>, <span style="color: #008800; font-weight: bold">None</span> <span style="color: #000000; font-weight: bold">or</span> string

Number of components to keep<span style="color: #333333">.</span> <span style="color: #008800; font-weight: bold">if</span> n_components <span style="color: #000000; font-weight: bold">is</span> <span style="color: #000000; font-weight: bold">not</span> <span style="color: #007020">set</span> <span style="color: #007020">all</span> components are kept:
    n_components <span style="color: #333333">==</span> <span style="color: #007020">min</span>(n_samples, n_features)
    <span style="color: #008800; font-weight: bold">if</span> n_components <span style="color: #333333">==</span> <span style="color: #FF0000; background-color: #FFAAAA">‘</span>mle<span style="color: #FF0000; background-color: #FFAAAA">’</span>, Minka<span style="color: #FF0000; background-color: #FFAAAA">’</span>s MLE <span style="color: #000000; font-weight: bold">is</span> used to guess the dimension <span style="color: #008800; font-weight: bold">if</span> <span style="color: #0000DD; font-weight: bold">0</span> <span style="color: #333333">&amp;</span>lt; n_components <span style="color: #333333">&amp;</span>lt; <span style="color: #0000DD; font-weight: bold">1</span>, 
    select the number of components such that the amount of variance that needs to be explained 
    <span style="color: #000000; font-weight: bold">is</span> greater than the percentage specified by n_components
</pre></div>

<p>Next, we just need to use the <code>.fit_transform()</code> in order to perform the dimensionality reduction.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">sklearn.decomposition</span> <span style="color: #008800; font-weight: bold">import</span> PCA <span style="color: #008800; font-weight: bold">as</span> sklearnPCA

sklearn_pca <span style="color: #333333">=</span> sklearnPCA(n_components<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">2</span>)
sklearn_transf <span style="color: #333333">=</span> sklearn_pca<span style="color: #333333">.</span>fit_transform(all_samples<span style="color: #333333">.</span>T)

plt<span style="color: #333333">.</span>plot(sklearn_transf[<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>,<span style="color: #0000DD; font-weight: bold">0</span>],sklearn_transf[<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>,<span style="color: #0000DD; font-weight: bold">1</span>],\
     <span style="background-color: #fff0f0">&#39;o&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;blue&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class1&#39;</span>)
plt<span style="color: #333333">.</span>plot(sklearn_transf[<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>,<span style="color: #0000DD; font-weight: bold">0</span>], sklearn_transf[<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>,<span style="color: #0000DD; font-weight: bold">1</span>],\
     <span style="background-color: #fff0f0">&#39;^&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;red&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class2&#39;</span>)

plt<span style="color: #333333">.</span>xlabel(<span style="background-color: #fff0f0">&#39;x_values&#39;</span>)
plt<span style="color: #333333">.</span>ylabel(<span style="background-color: #fff0f0">&#39;y_values&#39;</span>)
plt<span style="color: #333333">.</span>xlim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>ylim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>legend()
plt<span style="color: #333333">.</span>title(<span style="background-color: #fff0f0">&#39;Transformed samples with class labels from matplotlib.mlab.PCA()&#39;</span>)

plt<span style="color: #333333">.</span>draw()
plt<span style="color: #333333">.</span>show()
</pre></div>


<p></p>

<p><img src='../Images/pca_plot_5.png'></p>

<p>The plot above seems to be the exact mirror image of the plot from out step by step approach. This is due to the fact that the signs of the eigenvectors can be either positive or negative, since the eigenvectors are scaled to the unit length 1, both we can simply multiply the transformed data by (-1) revert the mirror image.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">sklearn_transf <span style="color: #333333">=</span> sklearn_transf <span style="color: #333333">*</span> (<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">1</span>)

<span style="color: #888888"># sklearn.decomposition.PCA</span>
plt<span style="color: #333333">.</span>plot(sklearn_transf[<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>,<span style="color: #0000DD; font-weight: bold">0</span>],sklearn_transf[<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>,<span style="color: #0000DD; font-weight: bold">1</span>],\
    <span style="background-color: #fff0f0">&#39;o&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;blue&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class1&#39;</span>)
plt<span style="color: #333333">.</span>plot(sklearn_transf[<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>,<span style="color: #0000DD; font-weight: bold">0</span>], sklearn_transf[<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>,<span style="color: #0000DD; font-weight: bold">1</span>],\
    <span style="background-color: #fff0f0">&#39;^&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;red&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class2&#39;</span>)
plt<span style="color: #333333">.</span>xlabel(<span style="background-color: #fff0f0">&#39;x_values&#39;</span>)
plt<span style="color: #333333">.</span>ylabel(<span style="background-color: #fff0f0">&#39;y_values&#39;</span>)
plt<span style="color: #333333">.</span>xlim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>ylim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>legend()
plt<span style="color: #333333">.</span>title(<span style="background-color: #fff0f0">&#39;Transformed samples via sklearn.decomposition.PCA&#39;</span>)
plt<span style="color: #333333">.</span>show()

<span style="color: #888888"># step by step PCA</span>
plt<span style="color: #333333">.</span>plot(transformed[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>], transformed[<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">0</span>:<span style="color: #0000DD; font-weight: bold">20</span>],\
    <span style="background-color: #fff0f0">&#39;o&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;blue&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class1&#39;</span>)
plt<span style="color: #333333">.</span>plot(transformed[<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>], transformed[<span style="color: #0000DD; font-weight: bold">1</span>,<span style="color: #0000DD; font-weight: bold">20</span>:<span style="color: #0000DD; font-weight: bold">40</span>],\
    <span style="background-color: #fff0f0">&#39;^&#39;</span>, markersize<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">7</span>, color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;red&#39;</span>, alpha<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.5</span>, label<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;class2&#39;</span>)
plt<span style="color: #333333">.</span>xlim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>ylim([<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">4</span>,<span style="color: #0000DD; font-weight: bold">4</span>])
plt<span style="color: #333333">.</span>xlabel(<span style="background-color: #fff0f0">&#39;x_values&#39;</span>)
plt<span style="color: #333333">.</span>ylabel(<span style="background-color: #fff0f0">&#39;y_values&#39;</span>)
plt<span style="color: #333333">.</span>legend()
plt<span style="color: #333333">.</span>title(<span style="background-color: #fff0f0">&#39;Transformed samples step by step approach&#39;</span>)
plt<span style="color: #333333">.</span>show()
</pre></div>


<p><img src='../Images/pca_plot_6.png'>
<img src='../Images/pca_plot_7.png'></p>

<p>Looking at the 2 plots above, the distributions along the component axes look identical, only the center of the data is slightly different.</p>

		&nbsp;<br>
     	  &nbsp;<br>   
        
					
	

		</div> 	<!-- close article -->



			</div><!--close main -->
		
	</div><!--close content -->
	




<!-- START Footer -->

	<div id ='footer'>	
        <center>
	<hr>
        <div style="height:20px;"></div>
        <p>HTML and JavaScripts were written by Sebastian Raschka <br>
        Copyright (C) 2013-2014 Sebastian Raschka</p>


	<!-- START Twitter API -->

 <a href="https://twitter.com/rasbt" class="twitter-follow-button" data-show-count="false" data-show-screen-name="false">Follow @rasbt</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
	<!-- END Twitter API -->
<div style="height:20px;"></div>

	<hr>

     
        
		 <div style="height:10px;"></div>
		</center>
	</div>
<!-- END Footer -->


<!-- Start DISQUS -->

 <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'sebastianraschka'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


<!-- Start Google Analytics -->
		<script type="text/javascript">

  		var _gaq = _gaq || [];
  		_gaq.push(['_setAccount', 'UA-38457794-1']);
  		_gaq.push(['_trackPageview']);

  		(function() {
    	var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    	ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    	var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  		})();
		</script>
<!-- End  Google Analytics -->

</body>
</html>
